---
title: Autonomous Mobile Robot
description: An academic project involving the development of a mobile robot that can autonomously navigate and search for objects within an arena. 
layout: post
---

This is a university project for a robotics course. It uses an Alphabot2 kit from Waveshare with a Raspberry Pi 4 Model B as the controller. 

The objective of this project is to develop an autonomous mobile robot that can localize itself within an arena, map out the positions of objects in the arena, plan a path to visit specific objects around the arena based on the map and navigate according to the path. 


The development of this project is split into a 4 different parts:
1. Simultaneous Localization and Mapping (SLAM)
2. Object Detection and Recognition using a Convolutional Neural Network (CNN)  <---- Currently here
3. Path Plannning 
4. Integration

# SLAM 
SLAM was performed using an Extended Kalman Filter (EKF). How it works is as follows:
- A total of 10 ArUco markers are placed around the 3.2m x 3.2m arena.
- The state of the system is represented by the robot pose and landmark (ArUco marker) positions.
- Based on the measurement of robot's motion, a new state is predicted using the motion model to track the robot's pose over time. 
- The robot observes the ArUco markers around the arena using a camera. If the marker has not been observed before, it is added to the map. If it has, the existing map is updated with an estimate of the marker's position.
- The state is updated based on the predicted state and the observed markers, correcting the robot pose and landmark positions. 
-  The prediction and updating steps are performed iteratively as the robot moves and observes the environment from different viewpoints. At each iteration, the estimate of the robot's pose and markers are refined. 

To ensure the accuracy of the SLAM, the robot's wheels and camera were calibrated to ensure that the motion model and the camera used for SLAM was as accurate as possible. 

# Object Detection and Recognition using a CNN
The next step is to train an object detector for fruits which represent possible goals in the arena. 

The chosen model was YOLOv8 and the dataset was generated by taking pictures of the fruits and arena using the robot's camera. These pictures were then superimposed them onto each other with randomized positions, orientations and sizes. The dataset is then further augmented by adding all sorts of effects such as flipping the image, rotating it, randomly cropping, adding more blur, changing the brightness, etc. 

The end result is a dataset consisting of 20k images used to train a YOLOv8 model for 100 epochs. 

# Path Planning
The goal is to generate a path for the robot to visit 3 out of 5 fruits (goals) on the arena in a specific order (that can be determined by the user). Along the way, the robot should also not collide with any objects or go out of bounds of the arena. 

For the second last milestone, there was also the added challenge of obstacle avoidance involving the other 2 'non-target' fruits whose positions are not initially known by the robot (hence not known by the path planning algorithm).

First thing was to implement waypoint navigation on the robot. This allows semi-autonomous movement by specifying a coordinate on the arena which the robot takes as input. The robot will then rotate to face the waypoint and drive in a straight line towards it. 

Next was to implement the actual path planning algorithm based on this waypoint navigation method. The initial plan was to implement the A* algorithm but due to the need for obstacle detection and avoidance, D* Lite was the final choice. 

The end result is a program that generates a series of coordinates specifying the path. The robot will visit each goal by moving to each coordinate in the list generated by the path planning algorithm. The requirement was that the robot has to stop within 0.5m of each fruit, so the path planning algorithm generates coordiantes in multiples of 0.2m which is half the grid size of the arena. 

# Integration 
Now it's time to put everything together. But of course, integration does not come easy. One main issue we had with our robot throughout the semester is that it is unable to drive straight. This is simply due to the fact that the Alphabot2 doesn't come with encoders, nor are we allowed to added them in. This means that we no way to precise control the motors. The two casters on the robot are also not balanced or smooth, making the robot bounce slightly when it moves and also adding quite a bit of friction.   

Navigating to specific coordinates requires a good amount of precision and accuracy in the robot's movement. So this caused a lot of issues as the robot would curve and mess up its own state estimation and also undershoot/overshoot the distance during waypoint navigation. 

In the end, we were able to make it behave somewhat well after a lot of trial and error with implementing various hacks and workarounds to make it work. 

One simple workaround was balancing the left and right wheel speeds. This helped quite a lot but was not always consistent. Another thing is tuning the EKF state estimate of the robot such that it 'syncs up' with the physical movement of the robot. 

As for the casters, there wasn't much to be done about it. They weren't smooth and lubrication did not help. When we tried to balance the casters by adding washers so they both touch the ground, they had so much friction that it cause the robot to drag against the ground a bit when it tried to drive forward. In the end, the casters were simply left to be slightly unbalanced. 
