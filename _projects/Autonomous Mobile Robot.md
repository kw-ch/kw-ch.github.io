---
title: Autonomous Mobile Robot
description: An academic project involving the development of a mobile robot that can autonomously navigate and search for objects within an arena. 
layout: post
---

This project uses an Alphabot2 kit from Waveshare with a Raspberry Pi 4 Model B as the controller. 


The development of this project is split into a 4 different parts:
1. Simultaneous Localization and Mapping (SLAM)
2. Object Detection and Recognition using a Convolutional Neural Network (CNN)  <---- Currently here
3. Path Plannning 
4. Integration

# SLAM 
SLAM was done by manually driving the robot around the arena (3.2m x 3.2m in size) to look for ArUco markers and using them to determine the position of the robot on a map.

To achieve this, an Extended Kalman Filter and ArUco marker detector was implemented in Python.

# Object Detection and Recognition using a CNN
Since the end goal of this robot is to autonomously detect objects (fruits) placed around the arena, at this stage we train a CNN for object detection and recognition using YOLOv8 as the model. 

The dataset was generated by taking pictures of the fruits and arena using the robot's camera and then superimposing them onto each other with randomized positions and sizes. The dataset is then further augmented by adding all sorts of effects such as flipping the image, rotating it, randomly cropping, adding more blur, changing the brightness, etc. 

The end result is a dataset consisting of 20k images used to train a YOLOv8 model for 100 epochs. 

After loading the model into the main robot code, voila! We have a working fruit detector.

However, it does seem like the model is not very robust as it can sometimes detect random shapes in the background as fruits. But given that this is quite a controlled scenario for object detection (just 5 fruits), it should be fine (hopefully!).

# Path Planning
Then came the most complex part of the project by far, the path planning. The goal is to basically generate a path such that the robot will visit 3 out of 5 of the fruits on the arena in a specific order and also not collide with any of the ArUco markers and fruits along the way. There is also the added challenge of detecting and recognizing the other 2 'non-target' fruits as obstacles as well since the positions of those two fruits will not be initially known by the robot.

The initial plan was to implement the A* algorithm, a classic path planning algorithm. However, due to the need for obstacle detection and avoidance, the robot needs to be able to adapt its path as it detects the obstacles. A bit more research was done and we finally settled on the D* Lite algorithm which is essentially a dynamic version of A*. 

Then came the monumental task of implementing it from scratch. Luckily, there were plenty of online resources and Github repos of existing implementation that I could base my own implementation upon. Even then, this took a whole week to implement, test and debug (bye bye mid-sem break!). 

The end result is a program that generates a series of coordinates that the robot can navigate to step-by-step. The robot will slowly visit each fruit by moving through the list of coordinates generated by the path planning algorithm. As the requirement was that the robot has to stop within 0.5m of each fruit, we had the path planning algorithm generate coordinates in multiples of 0.2m which is half the grid size of the arena. 

# Integration and Fine-tuning
With each of the individual parts done, it was time to integrate them all together. 'Integrating' might be a bit of a stretch as there actually wasn't much to integrate in the first place since the SLAM and fruit mapping can be done manually via teleoperation. This basically means that after teleoperating the robot to map out positions of the ArUco markers and fruits, we can just feed the map into the D* Lite algorithm to plan a path to perform the fruit search task. 

Initially, we thought our work was done after writing the code to do the above but little did we know, these last 3 weeks of the semester will be our busiest yet. 

One of the main issues we had with our robot throughout the semester is that it is unable to drive straight. This is simply due to the fact that the Alphabot2 kit doesn't come with encoders, nor are we allowed to retrofit it with some. This means that we practically have no way of precisely controlling the motors driving this robot. To make matters, the two casters on the front and back of the robot aren't quite balanced or smooth, making the robot bounce slightly when it moves and also adding quite a bit of friction.   

Navigating to specific coordinates requires a good amount of precision and accuracy in the robot's movement. So the inability to have any form of accurate motor control caused a lot of issues for us. In the end, we were able to make it behave somewhat well but only after a lot of trial and error with implementing various hacks and workarounds to make it. 

One of the simplest things we did was just trying to balance the left and right wheel speeds. This helped quite a lot but was not always consistent due to the robot bouncing slightly every time it moves. Another thing we did was tuning the EKF state estimate of our robot such that it actually takes into account the inaccuracies in our robot movement. This helped tremendously in 'synchronizing' our robot's EKF state and its true pose. 

As for the imbalanced casters, we simply decided to live with it as had tried adding washers to balance them out. However, when both casters laid flat on the ground, we found that the casters had so much friction that it would actually momentarily stop the robot in place when it tried to drive forward. Even after some lubrication the problem still persisted so we simply left it such that the casters was just very slightly unbalanced. The robot would still bounce but significantly less than before. 
