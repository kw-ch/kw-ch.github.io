---
title: Autonomous Mobile Robot
description: An academic project involving the development of a mobile robot that can autonomously navigate and search for objects within an arena. 
layout: post
---

This project uses an Alphabot2 kit from Waveshare with a Raspberry Pi 4 Model B as the controller. 


The development of this project is split into a 4 different parts:
1. Simultaneous Localization and Mapping (SLAM)
2. Object Detection and Recognition using a Convolutional Neural Network (CNN)  <---- Currently here
3. Path Plannning 
4. Integration

# SLAM 
SLAM was done by manually driving the robot around the arena (3.2m x 3.2m in size) to look for ArUco markers and using them to determine the position of the robot on a map.

To achieve this, an Extended Kalman Filter and ArUco marker detector was implemented in Python.

# Object Detection and Recognition using a CNN
Since the end goal of this robot is to autonomously detect objects (fruits) placed around the arena, at this stage we train a CNN for object detection and recognition using YOLOv8 as the model. 

The dataset was generated by taking pictures of the fruits and arena using the robot's camera and then superimposing them onto each other with randomized positions and sizes. The dataset is then further augmented by adding all sorts of effects such as flipping the image, rotating it, randomly cropping, adding more blur, changing the brightness, etc. 

The end result is a dataset consisting of 20k images used to train a YOLOv8 model for 100 epochs. 

# Path Planning
The goal here is to generate a path such that the robot will visit 3 out of 5 of the fruits on the arena in a specific order (as determined by the user) and also not collide with any objects along the way or go out of bounds. There is also the added challenge of obstacle avoidance involving the other 2 'non-target' fruits as their positions of those will not be initially known by the robot.

The initial plan was to implement the A* algorithm. However, due to the need for obstacle detection and avoidance, D* Lite was our final choice. 

Implementing it from scratch will be tough. Luckily, there's plenty of online resources and Github repos of existing implementations that could be used as refeerence.

The end result is a program that generates a series of coordinates that the robot can navigate to one-by-one. The robot will slowly visit each fruit by moving through the list of coordinates generated by the path planning algorithm. The requirement was that the robot has to stop within 0.5m of each fruit, so the path planning algorithm generates coordiantes in multiples of 0.2m which is half the grid size of the arena. 

# Integration 
Now it's time to put everything together. But of course, integration does not come easy. One main issue we had with our robot throughout the semester is that it is unable to drive straight. This is simply due to the fact that the Alphabot2 doesn't come with encoders, nor are we allowed to added them it. This means that we no way to precise control the motors. The two casters on the robot are also not balanced or smooth, making the robot bounce slightly when it moves and also adding quite a bit of friction.   

Navigating to specific coordinates requires a good amount of precision and accuracy in the robot's movement. So this caused a lot of issues. In the end, we were able to make it behave somewhat well but only after a lot of trial and error with implementing various hacks and workarounds to make it. 

One simple workaround was just trying to balance the left and right wheel speeds. This helped quite a lot but was not always consistent due to the robot bouncing slightly every time it moves. Another thing is tuning the EKF state estimate of the robot such that it actually takes into account the inaccuracies in the robot movement. This helped tremendously in 'synchronizing' the robot's EKF state and its true pose. 

As for the casters, there wasn't much to be done about it. They weren't smooth and lubrication did not help. When we tried to balance the casters by adding washers so they both touch the ground, they had so much friction that it cause the robot to drag against the ground a bit when it tried to drive forward. In the end, the casters were simply left to be slightly unbalanced. 
