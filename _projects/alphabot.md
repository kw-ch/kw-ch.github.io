---
title: Autonomous Mobile Robot
description: A mobile robot that can autonomously navigate and search for objects within an arena. 
layout: post
tags:
  - project
---

This is a university project for a robotics course. It uses an Alphabot2 kit from Waveshare with a Raspberry Pi 4 Model B as the controller. 

The main objective is to develop an autonomous mobile robot that can localize itself within an arena and navigate to 3 target objects (the targets are fruits) and avoid obstacles such as ArUco markers and non-target fruits (there were 5 fruits total, so any 3 of them can be targets and the remaining 2 are obstacles). 

Here is an example of how the arena looks like:
<p align="center">
  <img src="/assets/alphabot_arena.jpg">
</p>

The development of this project can be split into a 4 different parts:
1. Simultaneous Localization and Mapping (SLAM)
2. Object Detection and Recognition with YOLOv8
3. Path Plannning 
4. Integration

# SLAM 
SLAM was performed using an Extended Kalman Filter (EKF). How it works is as follows:
- A total of 10 ArUco markers are placed around the 3.2m x 3.2m arena.
- The state of the system is represented by the robot pose and marker positions.
- A new state is predicted using a model of the robot's motion to track the robot's pose over time.
- The robot observes the markers around the arena using the camera and estimates its distance from the marker. If the marker was not seen before, it is added to the map. If it has, the existing map is updated with a new estimate of the marker's position.
- The state is updated based on the predicted state and the observed markers to correct the robot pose and landmark positions. 
-  The prediction and updating steps are repeated as the robot is moved and observes the environment from different viewpoints, slowly refining the robot's internal map and localization

The robot's wheels and camera were calibrated many times to ensure that the motion model and the camera used for SLAM was as accurate as possible. 

# Object Detection and Recognition with YOLOv8
The next step is to train a YOLOv8 model to make an object detector for the fruits which are the target objects in the arena. 

The dataset was generated by taking pictures of the fruits and arena using the robot's camera. These pictures were then used to generate a dataset by randomly superimposing various fruit pictures onto various arena pictures that served as the background. The dataset was further augmented by randomizing the position, orientation and size of the fruits as well as adding all sorts of effects such as blur, brightness, contrast, random cropping, etc. 

The end result is a dataset consisting of 20k images to train a YOLOv8 model for 100 epochs. 

# Path Planning
The goal is to generate a path for the robot to visit 3 out of 5 fruits (goals) on the arena in a specific order (that can be determined by the user). Along the way, the robot should also not collide with any objects or go out of bounds of the arena. 

First thing was implementing waypoint navigation on the robot. This allows semi-autonomous movement by specifying a coordinate on the arena to which the robot will navigate to by first rotating to face the waypoint and then driving in a straight line towards it. 

Next was to implement the path planning algorithm based on this waypoint navigation method. One of the restrictions imposed was that the positions of the non-target fruits cannot be known ahead of time. So because there is a need to dynamically adjust the planned path due to the presence of new obstacles, the D* Lite path planning algorithm was the final choice. 

The end result is a function that generates a series of coordinates which specifies the path that the robot will take. The robot will move through each coordinate in the list and will occasionally relocalize itself by doing a 360Â° scan of the arena. The robot has to stop at least within 0.5m of each target fruit, so we made the generated coordinates have a step size of 0.2m.

# Integration and fine-tuning
Now it's time to put everything together. As the code was mostly modular, integrating everything was relatively painless. Our continuous integration efforts throughout the semester also helped significantly with the final integration. 

All that's really left is to fine-tune things and make them work as best as possible. One main issue we had with our robot throughout the semester is that it is unable to drive straight. The Alphabot2 doesn't come with encoders, nor are we allowed to add them in. This means that precisely controlling the motors is a challenging task.   

Navigating to specific coordinates requires a good amount of precision and accuracy in the robot's movement. So this caused a lot of issues as the robot would curve and mess up its own localisation. 

As hardware modifications weren't allowed, we tried a few software workarounds. One was tuning the left and right wheel speeds. As they may not necessarily turn at the same speed even if given the same speed input, this helped to balance the two wheel speeds and make it not curve. Another workaround was to modify the EKF state estimation such that the motion model was more in line with the physical movement of the robot.  